Sampling distribution of the MLE {#sec:sampling-distribution}
================================
The MLE $\hat{\theta}$ as a function of a random sample
of masked data, and is thus a random vector.

A primary statistic of an estimator is its *confidence interval*.
A $(1-\alpha)100\%$ confidence interval for $\theta_j$ may be estimated with
$\hat\theta_j \pm z_{1-\alpha/2} \sqrt{\hat{V}_{j j}}$.
We provide a method for doing this calculation:
```{r}
confint(estimate)
```

Theoretically, $\hat{\theta}$ converges in distribution
to the multivariate normal with a mean $\theta$ and 
variance-covariance matrix $J^{-1}(\theta)$.
We may estimate $\theta$ with $\hat\theta$A and we may estimate the
variance-covariance with the inverse of the
observed Fisher matrix, which is given by
$$
    J(\hat{\theta}) = -\nabla^2 l|_{\hat{\theta}}.
$$
Thus, approximately,
$$
    \hat{\theta} \sim \mathcal{N}(\theta,J^{-1}(\hat{\theta})).
$$

Asymptotically, $\hat{\theta}$ is the UMVUE, i.e.,
it is unbiased and obtains the minimum sampling variance.
An estimate of the variance-covariance may be obtained with:
```{r}
(V.hat <- vcov(estimate))
```


## Bias and mean squared error
We would like to measure the accuracy and precision of $\hat{\theta}$.
In statistical literature, the bias
$$
\operatorname{b}(\hat{\theta}) = E(\hat{\theta}) - \theta
$$
is a measure of accuracy and variance is a measure of precision.

The mean squared error, denoted by $\operatorname{MSE}$, is a measure of
estimator error that incorporates both the bias and the variance,
$$
\operatorname{MSE}(\hat{\theta}) =
    \operatorname{trace}\bigl(\operatorname{vcov}(\hat{\theta})\bigr) +
    \operatorname{b}^2(\hat{\theta}).
$$

Since $\hat{\theta}$ is asymptotically unbiased and minimum variance,
$$
\lim_{n \to \infty} \operatorname{MSE}(\hat{\theta}) =
    \operatorname{trace}\bigl(\operatorname{vcov}(\hat{\theta})\bigr).
$$
Thus, for sufficiently large samples, $\operatorname{MSE}(\hat{\theta})$ is
approximately given by the `trace` of the estimated variance-covariance matrix:
```{r}
(mse <- sum(diag(V.hat)))
```

If we have a sample of $n$ MLEs, $\hat{\theta}^{(1)},\ldots,\hat{\theta}^{(n)}$,
then we may estimate both the bias and the MSE respectively with the statistics
$$
\hat{\operatorname{b}} = \frac{1}{n} \sum_{i=1} \hat{\theta}^{(i)} - \theta
$$
and
$$
\widehat{\operatorname{MSE}} = \frac{1}{n}
    \sum_{i=1}^n (\hat{\theta}^{(i)} - \theta)
                 (\hat{\theta}^{(i)} - \theta)'.
$$
We may then compare these statistics, $\hat{\operatorname{b}}$ and
$\widehat{\operatorname{MSE}}$, with the asymptotic bias $({0})$ and the
asymptotic $\operatorname{MSE}$.

Let us compute estimates of the bias, variance-covariance, and mean squared
error as a function of sample size $n$ using Monte Carlo simulation.
Note that this is similar to the Bootstrap, except we know $\theta$.

```{r, fig.width=5,fig.height=5}
#stats1 <- readr::read_csv2("./data-raw/exp_series_stats_1.csv")
```

## Estimating the variance-covariance using the Bootstrap method
Alternatively, we could estimate $\theta$ with $B$ simulated draws from
the MLEs that satisfy
$$
\operatorname{argmax}_{\theta \in \Omega} \ell(\theta|\mathcal{D_i})
$$
where $\mathcal{{D_i}}$ is a random sample from the empirical distribution
$\{(S_i,\delta_i,C_i)\}_1^n$. We call this the *Bootstrap*.

Assuming the above solution to the MLE equation is _unique_, this gives us a
single point $\hat{\theta}_{(i)}$ when conditioned on the simulated masked
data ${D_i}$.


```{r  eval=F}
bias(estimate)
```


Candidate sets that generate non-unique MLEs {#sec:non-unique-mle}
============================================
In some cases, the MLE may be non-unique due to a small sample size.
However, there are cases where as the sample size $n \to \infty$, the MLE
remains non-unique, i.e., it is not guaranteed to converge to any value.
One way in which this can occur for a series system with masked component causes
of failure is given by the following setup.

Let the exponential series system consist of $m=3$ components. Components
$1$ and $2$ are on the same circuit board, and component $3$ is on another
circuit board.
Whenever the series system fails, the potential components that may have caused
the failure is determined by replacing the entire circuit board.
Thus, when compoent $1$ or $2$ fails, the series system is fixed by replacing
the circuit board they both reside on, and thus the candidate set is 
$\{1,2\}$. If component $3$ fails, the candidate set is $\{3\}$.
In this case, the log-likelihood function is given by
$$
\ell(\lambda_1,\lambda_2,\lambda_3) = \eta_3 \log(\lambda_3) +
    \eta_{1 2} \log(\lambda_1+\lambda_2) -
    (\eta_3+\eta_{1 2}) \bar{s} (\lambda_1+\lambda_2+\lambda_3),
$$
where $\eta_3$ denotes the number of observations in which the candidate
set is $\{3\}$ and $\eta_{1 2}$ denotes the number of observations in which
the candidate set is $\{1,2\}$.

Let $\hat{\lambda}_1 = 1$, $\hat{\lambda}_2 = 2$, and $\hat{\lambda}_3 = 3$.
Then,
$$
\ell(1,2,3) = \eta_3 \log(3) +
    \eta_{1 2} \log(1+2) -
    (\eta_3+\eta_{1 2}) \bar{s}(1+2+3).
$$
However, if we interchange the values for $\hat{\lambda}_1$ and $\hat{\lambda}_2$,
we get the same value for the log-likehood function.
Thus, any time $(\hat{\lambda}_1,\hat{\lambda}_2,\hat{\lambda}_3)$ is an MLE, so
is $(\hat{\lambda}_2,\hat{\lambda}_1,\hat{\lambda}_3)$.
This is known as *non-identifiability*, which means that two or more values of
the parameters result in the same likelihood of the observed data.

To find an MLE, we take the derivative of $\ell$ with respect to the parameters,
obtaining the simultaneous equations
\begin{align}
\hat\lambda_1 + \hat\lambda_2 &= \frac{\eta_{1 2}}{(\eta_{1 2} + \eta_3) \bar{s}}\\
\hat\lambda_3                 &= \frac{\eta_3}{(\eta_{1 2} + \eta_3) \bar{s}}
\end{align}
We see that
$$
\hat\lambda_1 + \hat\lambda_2 =\frac{\eta_{1 2}}{(\eta_{1 2} + \eta_3) \bar{s}}
$$
defines a *line*, and thus any point on this line is an MLE of $\hat\lambda_1$
and $\hat\lambda_2$.

As a demonstration of this occurrence, we run $N = 500000$ simulations for an
exponential series system with a true parameter $\lambda = (2,3,2.5)'$, starting
at a random location with the parameter space $\Omega$.
In Table \ref{mytable}, we show a small sample of the generated masked data.

```{r mytable, echo=F, cache=T, eval=F}
library(dplyr)
library(ggplot2)
library(md_series_system)
library(algebraic.mle)
library(cowplot)

n <- 10000
m <- 3
theta <- c(2,3,2.5)

md_block_candidate_m3 <- function(md)
{
    block <- function(k)
    {
        if (k == 1)
            return(c(T,T,F))
        if (k == 2)
            return(c(T,T,F))
        if (k == 3)
            return(c(F,F,T))
    }

    n <- nrow(md)
    x <- matrix(nrow=n,ncol=3)
    for (i in 1:n)
        x[i,] <- block(md$k[i])

    x <- tibble::as_tibble(x)
    colnames(x) <- paste0("x",1:3)
    md %>% dplyr::bind_cols(x)
}

md.nu <- tibble(t1=stats::rexp(n,theta[1]),
                t2=stats::rexp(n,theta[2]),
                t3=stats::rexp(n,theta[3])) %>%
    md_series_lifetime() %>%
    md_block_candidate_m3()

md.nu.tmp <- md.nu
md.nu.tmp$x1 <- as.integer(md.nu.tmp$x1)
md.nu.tmp$x2 <- as.integer(md.nu.tmp$x2)
md.nu.tmp$x3 <- as.integer(md.nu.tmp$x3)
#head(round(md.nu.tmp,digits=3),n=10)

knitr::kable(
    head(md.nu.tmp,n=10),
    caption="The first 10 observations of simulated masked data for exponentially distributed component lifetimes")
```

We find an MLE for each, and do density plots for $\hat\lambda_1$ and
$\hat\lambda_2$ on the left side and $\hat\lambda_3$ on the right side in
Figure \ref{fig:non-unique}.

```{r non-unique, eval=F, fig.cap="The left figure shows the MLE for lambda1 and lambda2 is centered around the line lambda1 + lambda2 = 5, and the right figure shows that the MLE for lambda3 is highly concentrated around 2.5.",fig.show="hold",echo=F,warning=F,message=F,cache=T}
loglike.nu.exp <- md_loglike_exp_series_C1_C2_C3(md.nu)
N <- 1000
loglikes <- numeric(N)
theta.nus <- matrix(nrow=N,ncol=3)
for (i in 1:N)
{
    theta.nu <- mle_gradient_ascent(
        l=loglike.nu.exp,
        theta0=runif(3,.1,10),
        stop_cond=function(x,y) abs(max(x-y)) < 1e-5)

    theta.nus[i,] <- point(theta.nu)
    loglikes[i] <- loglike.nu.exp(point(theta.nu))
}

dat <- data.frame(data.frame(x=theta.nus[,1], y=theta.nus[,2],z=loglikes))
plot1 <-ggplot(dat,aes(x=x,y=y)) +
    #geom_density2d() +
    geom_density_2d_filled() +
    #geom_contour()+
    labs(x="lambda1",y="lambda2") +
    xlim(0,5) +
    ylim(0,5)
dat2 <- data.frame(data.frame(x=theta.nus[,3],z=loglikes))

plot2 <-ggplot(dat2,aes(x=x)) +
    geom_density() +
    labs(x="lambda3") +
    xlim(2,3)

plot_grid(plot1, plot2, labels = "AUTO")
```




Estimating component cause
==========================
Another characteristic we may wish to estimate is the probability that a
particular component in an observation caused the system failure.

We wish to use as much information as possible to do this estimation.
We consider three cases:

1. We have masked data `data` with candidate sets and system failure times and seek
   to estimate the node failure probabilities of observations in this data.
   This case provides the most accurate estimates of the node probability failures,
   as have both system failure times and candidate sets as predictors of the node
   failure.

2. We have a new observation of a system failure time and an estimate of $\theta$
   from `data`. In this case, we cannot condition on candidate sets, since the
   observation does not include that information. However, we do have a system
   failure time.
   
3. We have an estimate of $\theta$ from `data` but wish to predict the node
   failure of a system that has failed, but we do not know when it failed.

We consider case 1 described above where we have masked data `data` that includes
both candidate sets and system failure times.

In this case, we are interested in
$$
    f_{K_i|C_i,T_i}(j|c_i,t_i,\theta) = \frac{h_j(t;{\theta_k})}{\sum_{j' \in c_i} h_{j'}(t_i;{\theta_{j'}})},
$$
which in the exponential series case simplifies to
$$
    f_{K_i|C_,T_i}(j|c_i,t_i,\theta) = \frac{{\theta_j}}{\sum_{j' \in c_i} {\theta_{j'}}}.
$$

We decorate `md` with this probability distribution with the decorator function
`md_series_component_failure_probability`, which accepts masked data as input
and returns the masked data with columns for component cause of failure
probabilities given by `k1`,...,`km`.

```{r  eval=F}
#h <- list(function(t) theta.hat[1],
#          function(t) theta.hat[2],
#          function(t) theta.hat[3])
#md %>% md_series_component_failure_probability_decorator(h)
```

We notice that every row over the columns `k1`, `k2`, and `k3`
given a specific candidate set are the same.
This is as expected, since in the case of the exponential series,
the component failure rates are constant with respect to system
failure time.

If we already had an estimate of $\theta$ and we sought to predict
the failed components from only system lifetime data, we would just let
the candidate sets contain all of the component indexes.

Also, observe that the component failure probabilities
$$
    \hat{ k}_i(\theta) = (\hat{k}_1,\hat{k}_2,\hat{k}_3)'
$$
is a random vector whose sampling distribution under the right conditions is a
multivariate normal whose $j$\textsuperscript{th} component is given by
$$
    \hat{k}_j \sim \mathcal{N}(f_{K_i|T_i}(j|t_i,\theta),\Sigma_i).
$$
We can simulate $n$ draws from $\hat{\theta}$ and then apply the above statistic of
interest, generating the data
$$
    \hat{ k}^{(1)},\ldots,\hat{ k}^{(n)}.
$$
