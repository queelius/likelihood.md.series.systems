---
title: "Using the masked.data package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using the masked.data package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

`masked.data` is an R package developed with the intention of providing a framework for estimating the parameters of latent node lifetimes from *masked data*.
Presently, we only provide support for series systems with node lifetimes in the `exponential`, `lomax`, `weibull`, and `pareto` parametric families.

Install the library with:
``` r
devtools::install_github("queelius/masked.data")
```

Once install, you may load the library and other dependencies with:

```{r setup,message=F}
# load the masked data library
library(masked.data)

# load other dependencies
library(purrr)
library(dplyr)
library(ggplot2)
library(tibble)
```

# Masked data

Masked data is given by an i.i.d. sample of system lifetime data and
a candidate set which contains the component that caused the system failure.
It may also be viewed as a predictor of the system lifetime, but we
we use the candidate sets in the sample to estimate component lifetimes.

`masked.data` contains several simulated masked data sets.
For example, a series system with $10$ exponentially distributed nodes
is stored in `exp_series_data_3`:

```{r}
print(exp_series_data_3,drop_latent=T,pprint=T)
```

## Candidate model $m_0$

In model $m_0$, the $j$-th observation in the masked data has
a candidate set $C_j$ that contains the failed component, in addition
to a random selection (without replacement) of $w_j-1$ additional components.

## Candidate model $m_1$: $\alpha$-masked candidates

In model $m_1$, the $j$-th observation in the masked data has a candidate set
$C_j$ described by the following:

1. $C_j$ contains the failed node with probability $\alpha_j$.
   Additionally, it contains a random selection (without replacement) of $w_j-1$
   non-failed nodes.

2. $C_j$ contains a random selection (without replacement) of $w_j$ non-failed nodes
   with probability $1-\alpha_j$.
   
# Exponential series system

The most straightforward series system to estimate is the series system with exponentially distributed node lifetimes.

Suppose an exponential series system with $m$ nodes is parameterized by $\theta = (3,4,5)'$.
Then, the $j$-th node has an exponentially distributed lifetime with a failure rate $\theta_j$.

## Candidate model $m_0$

In what follows, we consider the case of a series system with exponentially distributed
node lifetimes under candidate model $m_0$, as described earlier.
Later, we consider model $m_1$, the $\alpha$-masked candidate model.

The candidate model $M_0$ for masked data (of size $n=1000$) for this exponential
series system, with $w=2$ candidates for each observation, is given by:

```{r}
n <- 1000
theta <- c(3,4,5)
m <- length(theta)
w <- rep(2,n)
# todo: rename to md_exp_series_data_m0
md <- md_exp_series(n,theta,w,md_candidate_m0)
print(md,pprint=T,drop_latent=T)
```

### Log-likelihood of $\theta$ given masked data

```{r}
# kernel of log-likelihood
kloglik <- md_kloglike_exp_series_m0(md)
```

The log-likelihood function contains the maximum amount of information
about parameter $\theta$ given the sample of masked data `md`.

Suppose we do not know that $\theta = (3,4,5)'$.
With the log-likelihood, we may estimate $\theta$ with $\hat\theta$ by solving
$$
    \hat\theta = \operatorname{argmax}_{\theta} \operatorname{kloglik}(\theta),
$$
i.e., finding the point that *maximizes* the log-likelihood on
the observed sample `md`.
This is known as *maximum likelihood estimation*.

We typically solve for the MLE by solving
$$
    \nabla \operatorname{kloglik}(\theta) |_{\theta=\hat\theta} = 0.
$$

We use the iterative method known as the Fisher scoring algorithm to solve this,
$$
    \theta^{(n+1)} = \theta^n + I^{-1}(\theta^n) \nabla \operatorname{kloglik}(\theta^n),
$$
where $I$ is the information matrix (or observed information matrix).

We efficiently implement this algorithm with `md_mle_exp_series_m0`:
```{r}
mle <- md_mle_exp_series_m0(md) 
```

```{r}
theta.hat <- point(mle)
print(round(theta.hat,digits=3))
```

The `point` method takes an estimator object and obtains its
point estimate.
We see that $\hat\theta = (`r round(theta.hat,digits=3)`)$.

If we let the third argument in the log-likelhood function be fixed
at $\hat\theta_3 = `r round(theta.hat[3],digits=3)`$, then we may profile the
log-likelihood function over the first two parameters:

```{r,message=F}
prof <- function(theta1) { kloglik(c(theta1,theta.hat[2],theta.hat[3])) }
data <- tibble(x=seq(theta.hat[1]-3,theta.hat[1]+3,.05))
data$y <- numeric(nrow(data))
for (i in 1:nrow(data))
    data$y[i] <- prof(data$x[i])
data %>% ggplot(aes(x=x,y=y)) + geom_line() +
    geom_point(aes(x=theta.hat[1],prof(theta.hat[1])))
```

Due to sampling variability, different runs of the experiment
will result in different outcomes, i.e., $\hat\theta$ has a
sampling distribution.
We see that $\hat\theta \neq \theta$, but it is reasonably
close.
We may measure this sampling variability using the variance-covariance
matrix, bias, mean squared error (MSE), and confidence intervals.

### Variance-covariance matrix
The estimator $\hat\theta$ as a function of a random sample
of masked data has a sampling distribution.

#### Information matrix
Theoretically, $\hat\theta$ converges in distribution
to the multivariate normal with a mean $\theta$ and a
variance-covariance given the the inverse of the
observed Fisher matrix,
$$
    J(\theta) = -\nabla^2 \operatorname{loglik}(\theta).
$$
Since $J$ is a function of a random sample, it is a random
matrix.

However, when the nodes are exponentially distributed, we may
take the expectation
$$
    I(\theta) = -E(\nabla^2 \operatorname{loglik}(\theta)),
$$
and derive a closed-form solution.

The inverse of the information matrix $I$ (or $J$) computes the variance-covariance matrix.
For $3$-out-of-$3$ exponential series system, this results in
$$
V(\theta) =
    \frac{\lambda_1+\lambda_2+\lambda_3}{n}
    \begin{pmatrix}
        \lambda_1+\lambda_2+\lambda_3 & -\lambda_3                    & -\lambda_2\\
        -\lambda_3                    & \lambda_1+\lambda_2+\lambda_3 & -\lambda_1\\
        -\lambda_2                    & -\lambda_1                    & \lambda_1+\lambda_2+\lambda_3
    \end{pmatrix}.
$$

Asymptotically, $\hat\theta$ is the UMVUE, i.e.,
it is unbiased and obtains the minimum sampling variance.
An estimate of the variance-covariance $V$ may be obtained with:
```{r}
(V.hat <- vcov(mle))
```

#### Bootstrap: sample mean and sample variance-covariance
If we know the generative model, $\theta$ may be estimated $M$
times, $\hat\theta^{(1)},\ldots,\hat\theta^{(M)}$, and then 
the sample mean and sample covariance are estimates of $\theta$ and $V$, respectively.

```{r, cache=T, eval=F}
M <- 500
theta.hats <- matrix(nrow=M,ncol=m)
for (i in 1:M)
    theta.hats[i,] <- as.vector(point(
        md_mle_exp_series_m0(md_exp_series(n,theta,w,md_candidate_m0))))

data.frame(theta1=c(theta[1],mean(theta.hats[,1])),
           theta2=c(theta[2],mean(theta.hats[,2])),
           theta3=c(theta[3],mean(theta.hats[,3])),
           row.names=c("true value", "mle"))

cov(theta.hats)
vcov(mle)
```


### Bias, MSE, and confidence intervals
We would like to measure the accuracy and precision of $\hat\theta$ is.
In statistical literature, the bias
$$
    \operatorname{b}(\hat\theta) = E(\hat\theta) - \theta
$$
is a measure of accuracy and variance is a measure of precision.

The mean squared error, denoted by $\operatorname{MSE}$, is a measure of
estimator error that incorporates both the bias and the variance,
$$
    \operatorname{MSE}(\hat\theta) = \operatorname{trace}(V) + \operatorname{b}^2(\hat\theta).
$$

Since $\hat\theta$ is asymptotically unbiased and minimum variance,
$$
    \lim_{n \to \infty} \operatorname{MSE}(\hat\theta) = \operatorname{trace}(V).
$$

Thus, for sufficiently large samples, $\operatorname{MSE}(\hat\theta)$ is approximately given by the `trace`
of the estimated variance-covariance matrix:
```{r}
(mse <- sum(diag(V.hat)))
```

Mean squared error as a function of sample size $n$:

```{r, fig.width=5,fig.height=5}
Ns <- seq(100, nrow(exp_series_data_1), 100)
mses.vcov <- numeric(length=length(Ns))
mses <- numeric(length=length(Ns))

for (i in 1:length(Ns))
{
    mses.vcov[i] <- sum(diag(vcov(md_mle_exp_series_m0(exp_series_data_1[1:Ns[i],]))))
    mses[i] <- mean((point(md_mle_exp_series_m0(exp_series_data_1[1:Ns[i],])) - theta)^2)
}

tibble(mse=mses,mses.vcov=mses.vcov,n=Ns) %>% ggplot() +
    geom_line(aes(x=n,y=mses,color="sample")) + 
    geom_line(aes(x=n,y=mses.vcov,color="theory")) +
    scale_color_manual(name="Legend",values=c("sample"="blue", "theory"="red"))
```



A primary statistic is the *confidence interval*.
A $(1-\alpha)100\%$ confidence interval for $\theta_j$ may be estimated with $\hat\theta_j \pm z_{1-\alpha/2} \sqrt{\hat{V}_{j j}}$.
We provide a method for doing this calculation:
```{r}
as_tibble(confint(mle)) %>% mutate(length=.[[2]]-.[[1]])
```

How does this compare to the confidence intervals given that all candidate sets in the sample have size $w=1$?
```{r}
as_tibble(confint(md_mle_exp_series_m0(
    md_exp_series(n=n,
                  theta=theta,
                  w=rep(1,n),
                  candidate_model=md_candidate_m0)))) %>%
    mutate(length=.[[2]]-.[[1]])
```

We see that the lengths of the confidence intervals are significantly shorter.

If all (or a large portion) of the candidate sets are of size $w=m$ for an $m$-out-of-$m$ system,
then the estimator we have described is not consistent.

### Estimating node failures from a sample
Another characteristic we may wish to estimate is the probability that a particular node in an observation
caused the system failure.

We wish to use as much information as possible to do this estimation.
We consider three cases:

1. We have masked data `md` with candidate sets and system failure times and seek
   to estimate the node failure probabilities of observations in this data.
   This case provides the most accurate estimates of the node probability failures,
   as have both system failure times and candidate sets as predictors of the node
   failure.

2. We have a new observation of a system failure time and an estimate of $\theta$
   from `md`. In this case, we cannot condition on candidate sets, since the
   observation does not include that information. However, we do have a system
   failure time.
   
3. We have an estimate of $\theta$ from `md` but wish to predict the node
   failure of a system that has failed, but we do not know when it failed.


```{r}
fk <- md_exp_series_node_failure_m0(theta.hat)
md %>% select(-c("w",starts_with("t."))) %>%
    md_series_node_failure_decorator_m0(fk)
```

This output looks reasonable.
We notice that every row over the columns `k.1`, `k.2`, and `k.3`
given a specific candidate set are the same.
This is as expected, since in the case of the exponential series,
the node failure rates are not a function of time (due to the
memoryless property).

If we already had an estimate of $\theta$ and we sought to predict
the failed nodes from only system lifetime data, we would just let
the candidate sets contain all of the nodes.

### Estimating system lifetime intervals from candidate sets

Suppose we do not know the system failure time, but have masked
data with candidate sets and we have already estimated the parameters
of the series system.

Then, we can predict system failure times given these candidate sets.
Observe that
$$
    f_{S|C}(s|c) = f_{S,C} / f_C(c)
$$
which simplifies to thinking about the series system as being over
the subset of nodes in $c$.
Thus,
$$
    S|c = \min\{T_j | j \in c\}.
$$

We provide an interval estimate for $S|c$, e.g., the shortest interval that
includes the system failure time with, say, $90\%$ probability.
In the case of the exponential series system, this is just
$$
    (0,F^{-1}_{S|c}(0.95)).
$$

We consider a larger and more complicated data set for this next example, `exp_series_data_3`.
Type `?exp_series_data_3` to learn more about it.
It has a true parameter value of $\theta = (3,5,4,6,7,2,8,9,10,11)'$.
We decorate its masked data with these upper and lower bounds and compare the result with the known system
lifetimes, column `s`:

```{r}
(theta.large.hat <- point(md_mle_exp_series_m0(exp_series_data_3)))
q.hat <- md_exp_series_system_failure_interval_m0(theta.large.hat,.85)
(data <- exp_series_data_3 %>% md_series_system_failure_decorator_m0(q.hat) %>%
    select(-c("w","k",starts_with("t."),starts_with("c."))) %>%
    mutate(contains = s >= s.lower && s <= s.upper))
```

We can compute the proportion that contain the system lifetime with:
```{r}
mean(data$contains)
```

We were expecting something closer to $0.85$, but instead we obtained a result of around $0.95$.
TODO: investigate

## Candidate model $m_1$

Here, we derive similar results for the $\alpha$-masked model.

TODO: finish this section, but provide fewer examples and details.

# Lomax series systems

```{r}
print(lomax_series_data_1,pprint=T,drop_latent=T)
```

TODO: fix likelihood function, the output seems wron -- the algorithms I've tried
either do not converge to a solution, or converge to the wrong solution.
TODO: plot the surface of the profile of the log-likelihood, like I did for exponential. If it seems to be large in
the wrong area (given that we roughly know what the MLE should be), find out why. If it just looks highly non-linear,
then look into random restarts and other iterative methods, although starting near the true parameter value in
the search should largely avoid the problem. If I can't figure it out, move to Weibull instead.



