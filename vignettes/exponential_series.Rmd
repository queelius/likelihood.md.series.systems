---
title: "Masked Data Likelihood Model: Components with Exponentially Distributed Lifetimes Arranged In Series Configuration"
output:
    rmarkdown::html_vignette:
        toc: true
vignette: >
  %\VignetteIndexEntry{Masked Data Likelihood Model: Components with Exponentially Distributed Lifetimes Arranged In Series Configuration}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
- \renewcommand{\v}[1]{\boldsymbol{#1}}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = TRUE,
  comment = "#>")

library(likelihood.model)
library(likelihood.model.series.md)
library(algebraic.mle)
library(algebraic.dist)
library(md.tools)
library(tidyverse)
library(devtools)
options(digits=4)
```


The R package `likelihood.md.series.system` is a framework for
estimating the parameters of latent component lifetimes from *masked data*
in a series system.

Exponentially Distributed Component Lifetimes {#expo}
=====================================================
Consider a series system in which the components have exponentially distributed
lifetimes.
The $j$\textsuperscript{th} component of the $i$\textsuperscript{th} has a
lifetime distribution given by
$$
    T_{i j} \sim \operatorname{EXP}(\lambda_j)
$$
for $j=1,\ldots,m$.
Thus, $\lambda = \bigl(\lambda_1,\ldots,\lambda_m\bigr)$.
The random variable $T_{i j}$ has a reliability, pdf, and hazard function
given respectively by
\begin{align}
    \label{eq:expo_reliability}
    R_j(t|\lambda_j)   &= \exp(-\lambda_j t),\\
    \label{eq:expo_pdf}
    f_j(t|\lambda_j)   &= \lambda_j \exp(-\lambda_j t),\\
    \label{eq:expo_haz}
    h_j(\cdot|\lambda_j) &= \lambda_j
\end{align}
where $t > 0$ is the lifetime and $\lambda_j > 0$ is the failure rate of the $j$-th component.

The lifetime of the series system composed of $m$ components with exponentially
distributed lifetimes has a reliability function given by
\begin{equation}
\label{eq:sys_expo_reliability_function}
R_{T_i}(t_i|\v\lambda) = \exp \biggl(-\sum_{j=1}^{m}{\lambda_j} t_i\biggr)
\end{equation}
where $t > 0$.
\begin{proof}
By Theorem \ref{thm:sys_reliability_function},
$$
    R_{T_i}(t_i;\v\lambda) = \prod_{j=1}^{m} R_j(t_i;\lambda_j).
$$
Plugging in the component reliability functions given by
Equation \eqref{eq:expo_reliability} obtains the result
\begin{align*}
R_{T_i}(t_i|\v\lambda) = \prod_{j=1}^{m}
    &= \exp(-\lambda_j t)\\
    &= \exp \biggl(-\sum_{j=1}^{m}{\lambda_j} t_i\biggr).
\end{align*}
\end{proof}

A series system with exponentially distributed lifetimes is also 
exponentially distributed.
\begin{theorem}
\label{thm:expo_series_family}
The random lifetime $T_i$ of a series system composed of $m$ components with 
exponentially distributed lifetimes is exponentially distributed with a failure 
rate that is the sum of the component failure rates,
$$
    T_i \sim \operatorname{EXP} \biggl(\sum_{j=1}^{m} \lambda_j\biggr).
$$
\end{theorem}
\begin{proof}
By Equation \eqref{eq:sys_expo_reliability_function}, the series system has a
reliability function in the family of the exponential distribution with a
failure rate that is the sum of the component failure rates.
\end{proof}

The series system's failure rate function is given by
\begin{equation}
\label{eq:expo_sys_failure_rate}
h(\cdot|\v\lambda) = \sum_{j=1}^{m} \lambda_j
\end{equation}
whose proof follows from Theorem \ref{thm:sys_failure_rate}.

We see that the failure rate $\v\lambda = \sum_{j=1}^n \lambda$ is *constant*,
consistent with the the exponential distribution being the only continuous
distribution that has a constant failure rate.

The pdf of the series system is given by
\begin{equation}
\label{eq:expo_sys_pdf}
f_{T_i}(t_i|\v\lambda) = \biggl( \sum_{j=1}^{m} {\lambda_j} \biggr) \exp
    \biggl(-\sum_{j=1}^{m}{\lambda_j} t_i\biggr)
\end{equation}
where $t_i > 0$ is the lifetime of the system.
\begin{proof}
By definition,
$$
f_{T_i}(t_i|\v\lambda) = h_{T_i}(t_i|\v\lambda) R_{T_i}(t_i|\v\lambda).
$$
Plugging in the failure rate and reliability functions given respectively by
Equations \eqref{eq:expo_sys_failure_rate} and \eqref{eq:expo_reliability} completes
the proof.
\end{proof}

The conditional probability that component $k$ is the cause of a system failure
at time $t$ is given by
\begin{equation}
\label{eq:expo_prob_K_given_S}
f_{K_i|T_i}(k|t,\v\lambda) = f_{K_i}(k|\v\lambda) =
    \frac{\lambda_k}{\sum_{p=1}^{m} \lambda_p}
\end{equation}
where $k \in \{1,\ldots,m\}$ and $t > 0$.
\begin{proof}
By Theorem \ref{thm:f_given_s_form_2},
$$
f_{K_i|T_i}(k|t,\v\lambda) = \frac{h_k(t|\lambda_k)}{h_{T_i}(t;\v\lambda)}.
$$
Plugging in the failure rate of the component indexed by $k$ and the failure
rate of the system given respectively by Equations \eqref{eq:expo_sys_failure_rate}
and \eqref{eq:expo_haz} completes the proof.
\end{proof}

Due to the constant failure rates of the components, $K_i$ and $T_i$ are
mutually independent.
The joint pdf of $K_i$ and $T_i$ is given by
\begin{equation}
\label{eq:expo_joint_k_s}
f_{K_i,T_i}(k,t|\v\lambda) = \lambda_k \exp \biggl(-\sum_{j=1}^{m}{\lambda_j} t\biggr)
\end{equation}
where $k \in \{1,\ldots,m\}$ and $t > 0$.
\begin{proof}
By definition,
$$
f_{K_i,T_i}(k,t|\v\lambda) =
    f_{K_i|T_i}(k|t,\v\lambda) f_{T_i}(t|\v\lambda).
$$
Plugging in the conditional probability and the marginal probability given
respectively by Equations \eqref{eq:expo_prob_K_given_S} and
\eqref{eq:expo_sys_pdf} completes the proof.
\end{proof}

Likelihood Model {#likelihood-model}
====================================

In this study, the system is a series system with $m$
components. The true DGP for the system lifetime is in the
exponential series system family, i.e., the component lifetimes are
exponentially and independently distributed and we denote the
true parameter value by $\theta$.

The principle object of study is $\v\lambda, which in the case of
the exponential series system family consists of $m$ rate (scale)
parameters for each component lifetime, $\v\lambda = (\lambda_1, \ldots, \lambda_m)'$.

We are interested in estimating the $\theta$ from masked data.
The masking comes in two independent forms:

- Censored system failure times, e.g., right-censoring. The system failure
time is the minimum of the component lifetimes, and it is right-censored
if the system failure time does not occur during the observation period,
$$
    T_i = \min\{\tau_i, T_{i 1}, \ldots, T_{i m}\},
$$
where $\tau_i$ is the right-censoring time for the $i$\textsuperscript{th}
observation and $T_{i 1},\ldots,T_{i m}$ are the component lifetimes
for the $i$th system.

- The cause of failure, the failed component, is masked. This masking
comes in the form of a candidate set $\mathcal{C}_i$ that, on average,
conveys information about the component cause of failure.

The candidate set $\mathcal{C}_i$ is a random variable that is a subset of
$\{1,\ldots,m\}$. The true DGP for the candidate set model has a general form
that may be denoted by
$$
    \Pr\{\mathcal{C}_i=c_i|T_1=j,\ldots,T_m,\theta,\text{other factors}\}.
$$

This is a pretty complicated looking model, and we are not even interested
in the DGP for candidate sets, except to the extent that it affects the
sampling distribution of the MLE for $\theta$.

In theory, given some candidate set model, we could construct a joint likelihood
function for the full model and jointly estimate the parameters of both the
candidate set model and $\theta$. In practice, however, this could be a very
challenging task unless we make some simplifying assumptions about the DGP for
candidate sets.

## Candidate set models
In every model we consider, we assume that the candidate set $\mathcal{C}_i$
is only a function of the component lifetimes $T_{i 1},\ldots,T_{i m}$, $\theta$,
and the right-censoring time $\tau_i$. That is, the candidate set $\mathcal{C}_i$
is independent of any other factors (or held constant for the duration of the
experiment), like ambient temperature, and these factors also have a neglible
effect on the series system lifetime and thus we can ignore them.

### Reduced likelihood model
In the Bernoulli candidate set model, we make the following assumptions about
how candidate sets are generated:

  - $C_1$: The index of the failed component is in the candidate set,
  i.e., $\Pr\{K_i \in \mathcal{C}_i\} = 1$, where
  $K_i = \arg\min_j \{ T_{i j} : j = 1,\ldots,m\}$.

  - $C_2$: The probability of $C_i$ given $K_i$ and $T_i$ is equally
  probable when the failed component varies over the components in the candidate
  set, i.e., $\Pr\{\mathcal{C}_i=c_i|K_i=j,T_i=t_i,\theta\} = \Pr\{C_i=c_i|K_i=j',T_i=t_i\}$ for
  any $j,j' \in c_i$.
    
  - $C_3$: The masking probabilities are conditionally independent of $\theta$
  given $K_i$ and $T_i$, i.e., $\Pr\{\mathcal{C}_i=c_i|K_i=j,T_i=t_i\}$ is not a
  function of $\theta$.

Using these simplifying assumptions, we can arrive at a reduced likelihood
function that only depends on $\theta$ and the observed data and as long as our
candidate set satisfies conditions $C_1$, $C_2$, and $C_3$, our reduced
likelihood function obtains the same MLEs as the full likelihood function.

We see that
$$
    \Pr\{\mathcal{C}_i=c_i,|K_i=j,T_i=t_i\} = g(c_i,t_i),
$$
since the probability cannot depend on $j$ by condition $C_2$ and cannot depend
on $\theta$ by condition $C_3$. Thus, we can write the likelihood function as
$$
    L(\theta) = \prod_{i=1}^n f_{T_i}(t_i|\theta) g(c_i,t_i).
$$

We show that $g(c_i,t_i)$ is proportional to
$$
    g(c_i,t_i) \propto \sum_{j \in c_i} f_j(t_i|\theta_j) \prod_{l=j,l \neq j}^m R_l(t_i|\theta_l),
$$
and thus 

Note, however, that different ways in which the conditions are met will yield
MLEs with different sampling distributions, e.g., more or less efficient estimators.

### Bernoulli candidate set model #1
This is a special case of the reduced likelihood model.
In this model, we satisfy conditions $C_1$, $C_2$, and $C_3$, but we include 
each of the non-failed components with a fixed probability $p$, $0 < p < 1$.

In the simplest case, $p = 0.5$, and candidate set $c_i$ has a probability
given by
$$
\Pr\{\mathcal{C}_i=c_i|K_i=j,T_i=t_i\} =
\begin{cases}
        (1/2)^{m-1} & \text{if $j \in c_i$ and $c_i \subseteq \{1,\ldots,m\}$} \\
        0 & \text{if $j \notin c_i$}.
\end{cases}
$$
\begin{proof}
Since there are $m-1$ non-failed components (the failed component $j$ is 
in $c_i$ with probability $1$), there are $2^(m-1)$ possible
candidate sets (the size of the power set of the non-failed component indexes).
Each of these candidate sets has equal probability of occurring, and thus
the probability of any particular candidate set is $1/2^(m-1)$.
\end{proof}

### Bernoulli candidate set model #2

Now, we remove condition $C_2$. We still assume conditions $C_1$ and $C_3$,
but we allow $C_i$ to depend on the failed component $K_i$, i.e.,
$$
    \Pr\{\mathcal{C}_i=c_i|K_i=j,T_i=t_i,\theta\} \neq \Pr\{C_i=c_i|K_i=j',T_i=t_i\}
$$
for $j,j' \in c_i$.

In this case, we can write the likelihood function as
$$
    L(\theta) = \prod_{i=1}^n f_{T_i}(t_i|\theta) \prod_{j=1}^m \Pr\{K_i=j|T_i=t_i\} \prod_{c_i \in \mathcal{C}_i} g(c_i,t_i,j).
$$


Simulation {#simulation}
========================

The most straightforward series system to estimate is the series system with
exponentially distributed component lifetimes.

Suppose an exponential series system with $m$ components is parameterized by
the following R code:

```{r}
theta <- c(1,     # component 1 failure rate
           1.1,   # 3
           0.95,  # 5
           1.15,  # 6
           1.1)   # 7

m <- length(theta)
```

So, in our study, $\theta = (`r theta`)'$.
The component assigned to index $j$ has an exponentially distributed
lifetime with a failure rate $\theta_j$, e.g., $\theta_2 = `r theta[2]`$ is the
failure rate of the component indexed by $2$.

Let's simulate generating the lifetimes of the $m = `r m`$ components for this series
system:
```{r}
set.seed(7231) # set seed for reproducibility
n <- 7500
comp_times <- matrix(nrow=n,ncol=m)
for (j in 1:m)
    comp_times[,j] <- rexp(n,theta[j])
comp_times <- md_encode_matrix(comp_times,"t")
print(comp_times,n=4)
```

Next, we use the function `md_series_lifetime_right_censoring` to decorate the
masked data with the right-censor-censoring time chosen by the probability
$\Pr\{T_i > \tau\} = 0.75$:
```{r right-censoring}
q <- 0.25
tau <- rep(-(1/sum(theta))*log(q),n)
data <- comp_times %>% md_series_lifetime_right_censoring(tau)
print(data,n=4,drop_latent=TRUE)
```

## Masked component cause of failure
We simulate candidate sets using the Bernoulli candidate model with an
appropriate set of parameters to satisfy conditions $C_1$, $C_2$, and $C_3$:
```{r bernoulli-cand, warning=F, message=F}
p <- .3
data <- data %>% md_bernoulli_cand_c1_c2_c3(p)
print(data[,paste0("q",1:m)],n=4)
```

Now, to generate candidate sets, we sample from these probabilities:
```{r cand-sampler}
data <- data %>% md_cand_sampler()
print(md_boolean_matrix_to_charsets(data,drop_set=TRUE),drop_latent=TRUE,n=6)
```

We see that after dropping latent (unobserved) columns, we only have the right
censoring time, right censoring indicator, and the candidate sets. (Note that
this time we showed the candidate sets in a more friendly way using
`md_boolean_matrix_to_charsets`.)

Likelihood Model {#likelihood-model}
================
The likelihood model is a statistical model that describes the distribution of
the observed data as a function of the parameters of interest.

We construct a likelihood model for the masked data model with exponentially
distributed component lifetimes with the following code:

```{r likelihood-model}
model <- exp_series_md_c1_c2_c3()
```


Maximum likelihood estimation {#mle}
====================================
The log-likelihood for our masked data model when we assume Conditions
\ref{cond:c_contains_k}, \ref{cond:equal_prob_failure_cause}, and
\ref{cond:masked_indept_theta} is given by
\begin{equation}
\label{eq:}
\ell(\lambda) =
    \sum_{i=1}^{n} (1-\delta_i) \log \biggl(\sum_{j \in c_i} \lambda_j \biggr) -
    \biggl( \sum_{i=1}^{n} s_i \biggr)
    \biggl( \sum_{j=1}^{m} \lambda_j \biggr).
\end{equation}
\begin{proof}
By Equation \eqref{eq:loglike}, 
$$
\ell(\lambda) = \sum_{i=1}^n \log R(s_i;\lambda) + \sum_{i=1}^n (1-\delta_i)
    \log \biggl\{ \sum_{k\in c_i} h_k(s_i;{\lambda_k}) \biggr\}.
$$
Plugging in the component failure rate and system reliability functions given
respectively by Equations \eqref{eq:expo_haz} and
\eqref{eq:sys_expo_reliability_function} and simplifying completes the proof.
\end{proof}

The set of solutions to the MLE equations must be stationary points, i.e., a
point at which the score function of type $\mathbb{R}^m \mapsto \mathbb{R}^m$
is zero.
The $j$-th component of the output of the score function is given by
\begin{equation}
\label{eq:score_expo_j}
\frac{\partial \ell}{\partial \lambda_p} = 
  \sum_{i=1}^{n} \biggl( \sum_{j \in c_i} \lambda_j \biggr)^{-1}
  1_{\{p \in c_i \text{ and } \delta_i = 0\}} - \sum_{i=1}^{n} s_i.
\end{equation}

We may find an MLE by solving the maximum likelihood equation \eqref{eq:mle_eq},
i.e., a set of (stationary) points satisfying
$$
\frac{\partial \ell}{\partial \lambda_j}\Biggr|_{\hat\lambda_j} = 0
$$
for $j=1,\ldots,m$.
We approximate a solution to this problem by using the iterative
Newton-Raphson method as described in Section \ref{sec:iterative}.

The Newton-Raphson method needs the observed information matrix, which is a
function of $\lambda$ of type $\mathbb{R}^m \mapsto \mathbb{R}^{m \times m}$.
The $(j,k)$-th element of $J(\lambda)$ is given by
\begin{equation}
\label{eq:info_expo}
\frac{\partial^2 \ell}{\partial \lambda_j \partial \lambda_k} = 
  \sum_{i=1}^{n} \biggl( \sum_{j \in c_i} \lambda_j \biggr)^{-2}
  1_{\{j,k \in c_i \text{ and } \delta_i = 0\}}.
\end{equation}

## Log-likelihood of $\theta$ given masked data

The reduced log-likelihood function (the log of the kernel of the likelihood
function) is given by
$$
\ell(\theta|\text{data}) =
    -\left(\sum_{i=1}^{n} t_i\right)
    \left(\sum_{j=1}^{m} \theta_j\right) +
    \sum_{i=1}^{n} (1-\delta_i)\log\left(\sum_{j \in c_i} \theta_j\right).
$$

We compute the log-likelihood function as a function of the masked data `data` with:
```{r loglike-function}
ll <- loglik(model)
print(ll)
```
Note that `md_loglike_exp_series_C1_C2_c3` is implemented using minimally
sufficient statistics, which improves the computational efficiency of
evaluating the log-likelihood.

The log-likelihood function contains the maximum amount of information
about parameter $\theta$ given the sample of masked data `data` satisfying
conditions $C_1$, $C_2$, and $C_3$.

Suppose we do not know that $\theta = (`r theta`)'$.
With the log-likelihood, we may estimate $\theta$ with $\hat\theta$ by solving
$$
\hat{\theta} = \operatorname{argmax}_{\theta \in \Omega} \ell(\theta),
$$
i.e., finding the point that *maximizes* the log-likelihood on
the observed sample `data`.
This is known as *maximum likelihood estimation* (MLE).
We typically solve for the MLE by solving
$$
\nabla \ell|_{\theta=\hat{\theta}} = 0.
$$

A popular choice is gradient ascent, which is an iterative method
based on the update rule
$$
\theta^{(n+1)} = \theta^n + \eta \ell(\theta^n),
$$
where $\eta$ is the learning rate.

We'll show a simple way to solve this using gradient ascent.
First, we need the gradient function.
```{r score-function, message=F, warning=F}
grad <- score(model)
print(grad)
```
This is a custom version for computing the gradient of the log-likelihood
`md_loglike_exp_series_C1_C2_C3`. If `md_score_exp_series_C1_C2_C3` was
not implemented, a simple numerical approximation of the gradient could be
computed instead. More more advanced techniques, like auto-gradient type packages,
may also be used, but for relatively small samples, it may not be worth the
time.

In what follows, we use [algebraic.mle](https://github.com/queelius/algebraic.mle)
to help solve the MLE equations and display various properties of the solution.

To solve the MLE equation, we use the `optim` function in R and the method
`BFGS`, which is a quasi-Newton method, which is a second-order local search
method. Thus, to guarantee convergence to a global maximum that is interior to
the support of the parameter space (assuming one exists), a good initial guess
near this point should be chosen.

```{r bfgs, warning=F, cache=TRUE, eval=F}
library(stats)
theta0 <- rep(1, m)
theta.hat <- stats::optim(theta0, fn = ll,
    method = "L-BFGS-B", lower = 1e-3, hessian = TRUE,
    control = list(fnscale = -1, maxit = 1000))
```



```{r, eval=F}

print(theta.hat$par)
print(theta.hat$value)

estimate <- algebraic.mle::mle_numerical(res)
```

Note that `mle_numerical` is a constructor for `mle` objects.
We can get a summary of the object with:
```{r, eval=F}
summary(estimate)
```

We let `theta.hat` be given by the `point` method, which obtains the point
$\hat{\theta}$.
```{r eval=F}
(theta.hat <- param(estimate))
```
We see that
$$
\hat{\theta} = ...
$$
Recall that the true parameter is $\theta = (`r theta`)'$.

Due to sampling variability, different runs of the experiment
will result in different outcomes, i.e., $\hat{\theta}$ has a
sampling distribution.
We see that $\hat{\theta} \neq \theta$, but it is reasonably
close.
We may measure this sampling variability using the variance-covariance
matrix, bias, mean squared error (MSE), and confidence intervals.


